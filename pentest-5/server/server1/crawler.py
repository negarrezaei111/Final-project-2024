import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from .models import Scan, VulnerabilityDetail, SubScan
from .utils import check_sql_injection, check_xss


def is_valid_url(url, base_url):
    parsed_url = urlparse(url)
    return bool(parsed_url.scheme and parsed_url.netloc and url.startswith(base_url))


def crawl(url, scan_id, scan_mode,max_depth=2):
    visited_urls = set()
    urls_to_visit = [(url, 0)]
    print("scan mode in scripts", scan_mode)

    if scan_mode == 'deep':
        max_depth = 4
    use_deep_scan = True if scan_mode == 'deep' else False
    try:
        parent_scan = Scan.objects.get(id=scan_id)
    except Scan.DoesNotExist:
        print(f"Scan with ID {scan_id} does not exist.")
        return visited_urls

    while urls_to_visit:
        current_url, depth = urls_to_visit.pop(0)


        parent_scan.refresh_from_db()
        if parent_scan.status in ['failed', 'completed'] or parent_scan.is_stopping:
            print(f"Scan with ID {scan_id} has been stopped or completed. Stopping crawl.")
            break

        if current_url in visited_urls or depth > max_depth:
            continue

        visited_urls.add(current_url)
        try:
            response = requests.get(current_url)
            response.raise_for_status()

            print(f"Crawling URL: {current_url} at depth {depth}")
            soup = BeautifulSoup(response.text, 'html.parser')

            sql_injection_results = check_sql_injection(current_url, use_deep_scan=use_deep_scan)
            xss_results = check_xss(current_url, use_deep_scan=use_deep_scan)

            vulnerabilities = sql_injection_results + xss_results

            sub_scan = SubScan.objects.create(
                crawled_url=current_url,
                scan_date=parent_scan.scan_date,
                is_vulnerable=bool(vulnerabilities),
                vulnerabilities=json.dumps(vulnerabilities),
                security_score=min(
                    [100 - (10 if 'sql' in v.get('description', '').lower() else 5) for v in vulnerabilities],
                    default=100),
                scan=parent_scan,
                depth=depth
            )

            for vulnerability in vulnerabilities:
                vuln_type = vulnerability.get('severity', 'Medium')
                VulnerabilityDetail.objects.create(
                    sub_scan=sub_scan,
                    parameter=vulnerability.get('parameter', 'N/A'),
                    payload=vulnerability.get('payload', 'N/A'),
                    description=vulnerability.get('description', 'No description provided'),
                    vuln_type=vuln_type
                )

            for link in soup.find_all('a', href=True):
                new_url = urljoin(current_url, link['href'])
                if is_valid_url(new_url, url):
                    urls_to_visit.append((new_url, depth + 1))

        except requests.RequestException as e:
            print(f"Failed to crawl URL {current_url}: {e}")

    return visited_urls
